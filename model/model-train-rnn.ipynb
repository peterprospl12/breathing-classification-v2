{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Model classes**",
   "id": "24d27a0b9153932a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T23:10:10.721114Z",
     "start_time": "2025-01-09T23:10:10.599923Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Model that uses LSTM network\n",
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "\n",
    "        # input_size - number of features in the input (20 MFCC coefficients)\n",
    "        self.lstm = nn.LSTM(input_size=20, hidden_size=256, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # Zwraca wszystkie ukryte stany\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Zwraca etykiety dla ka≈ºdej klatki\n",
    "        return x\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the sequence consisting of MFCCs and labels\n",
    "        sequence = self.data[idx]\n",
    "\n",
    "        # Extract MFCCs and labels from the sequence\n",
    "        mfcc_sequence = [item[0] for item in sequence]\n",
    "        labels = [item[1] for item in sequence]\n",
    "\n",
    "        # Convert the lists to PyTorch tensors\n",
    "        mfcc_sequence = torch.tensor(mfcc_sequence, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        # Return lists of MFCC ndarrays and labels\n",
    "        return mfcc_sequence, labels\n",
    "\n",
    "# Model that use GRU network\n",
    "class AudioClassifierGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioClassifierGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=20, hidden_size=256, num_layers=3, batch_first=True, dropout=0.2)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Swap the dimensions for GRU (batch, seq, feature)\n",
    "        _, hn = self.gru(x)  # hn contains the last hidden state\n",
    "        x = hn[-1]  # Take the last hidden state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Padding for sequence\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # Get the sequences and labels from the batch\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # Get the lengths of sequences\n",
    "    lengths = [seq.size(0) for seq in sequences]\n",
    "\n",
    "    # Get the maximum length\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    # Create padded sequences and labels\n",
    "    padded_sequences = torch.zeros(len(sequences), max_length, 20)\n",
    "    padded_labels = torch.zeros(len(sequences), max_length, dtype=torch.long)\n",
    "\n",
    "    # Fill the padded sequences and labels\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :seq.size(0), :] = seq\n",
    "        padded_labels[i, :len(labels[i])] = labels[i]\n",
    "\n",
    "    # Return padded sequences and labels\n",
    "    return padded_sequences, padded_labels"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data preprocessing**",
   "id": "f60c5ddf4c188baa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T22:46:47.781120Z",
     "start_time": "2025-01-09T22:43:35.548663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "REFRESH_TIME = 0.25  # seconds\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Directories with data\n",
    "exhale_dir = 'small-data/exhale'\n",
    "inhale_dir = 'small-data/inhale'\n",
    "silence_dir = 'small-data/silence'\n",
    "\n",
    "# Creating list of files\n",
    "exhale_files = [os.path.join(exhale_dir, file) for file in os.listdir(exhale_dir)]\n",
    "inhale_files = [os.path.join(inhale_dir, file) for file in os.listdir(inhale_dir)]\n",
    "silence_files = [os.path.join(silence_dir, file) for file in os.listdir(silence_dir)]\n",
    "train_data = []\n",
    "files_list = [exhale_files, inhale_files, silence_files]\n",
    "files_names = ['exhale', 'inhale', 'silence']\n",
    "\n",
    "# Amount of sequences of every class\n",
    "exhale_frames_size = 0\n",
    "inhale_frames_size = 0\n",
    "silence_frames_size = 0\n",
    "\n",
    "# Main loop to preprocess data into MFCCs\n",
    "for label, files in enumerate(files_list):\n",
    "\n",
    "    # Iterate through all files (potentially longer audio recording with different classes)\n",
    "    for file in files:\n",
    "\n",
    "        # Load vaw file and keep file's sampling rate\n",
    "        y, sr = librosa.load(file, mono=True)\n",
    "\n",
    "        # Calculate chunk size\n",
    "        chunk_size = int(sr * 0.05)  # 0.25 seconds for example\n",
    "\n",
    "        # List of MFCCs for every data sequence (it will be a list of MFCC coefficients lists)\n",
    "        mfcc_sequence = []\n",
    "\n",
    "        # Iterate trough every 0.25s audio chunk\n",
    "        for i in range(0, len(y), chunk_size):\n",
    "\n",
    "            # Get frames of current chunk\n",
    "            frame = y[i:i + chunk_size]\n",
    "\n",
    "            if len(frame) == chunk_size:  # Ignore the last frame if it's shorter\n",
    "\n",
    "                # Calculate MFCCs (it will be a vector of MFCC coefficients - a vector of vectors)\n",
    "                mfcc = librosa.feature.mfcc(y=frame, sr=sr)  # Default n_mfcc = 20 (20 coefficients per subframe)\n",
    "\n",
    "                # Because we have a list of MFCC vectors, we can calculate the mean of every coefficient so we get just one set of coefficients for every 0.25s chunk\n",
    "                mfcc_mean = mfcc.mean(axis=1)\n",
    "\n",
    "                # Append the mean of MFCCs to the list of MFCCs for the current data sequence\n",
    "                mfcc_sequence.append((mfcc_mean, label))\n",
    "\n",
    "        print(file)\n",
    "\n",
    "        if mfcc_sequence:\n",
    "\n",
    "            # Append the list of MFCCs for the current data sequence to the list of all data sequences\n",
    "            train_data.append(mfcc_sequence)\n",
    "\n",
    "    # Print the amount of sequences for every class\n",
    "    if label == 0:\n",
    "        exhale_frames_size = len(train_data)\n",
    "        print(\"Exhale frames size: \", exhale_frames_size)\n",
    "    elif label == 1:\n",
    "        inhale_frames_size = len(train_data) - exhale_frames_size\n",
    "        print(\"Inhale frames size: \", inhale_frames_size)\n",
    "    else:\n",
    "        silence_frames_size = len(train_data) - exhale_frames_size - inhale_frames_size\n",
    "        print(\"Silence frames size: \", silence_frames_size)"
   ],
   "id": "2e259d6856399c02",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasz/Pulpit/breathing-classification-v2/.venv/lib/python3.12/site-packages/librosa/core/spectrum.py:256: UserWarning: n_fft=2048 is too large for input signal of length=1102\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small-data/exhale/master_exhale3.wav\n",
      "small-data/exhale/master_exhale6.wav\n",
      "small-data/exhale/master_exhale5.wav\n",
      "small-data/exhale/master_exhale15.wav\n",
      "small-data/exhale/master_exhale37.wav\n",
      "small-data/exhale/master_exhale21.wav\n",
      "small-data/exhale/master_exhale41.wav\n",
      "small-data/exhale/master_exhale25.wav\n",
      "small-data/exhale/master_exhale43.wav\n",
      "small-data/exhale/master_exhale39.wav\n",
      "small-data/exhale/master_exhale1.wav\n",
      "small-data/exhale/master_exhale22.wav\n",
      "small-data/exhale/master_exhale11.wav\n",
      "small-data/exhale/master_exhale49.wav\n",
      "small-data/exhale/master_exhale19.wav\n",
      "small-data/exhale/master_exhale23.wav\n",
      "small-data/exhale/master_exhale10.wav\n",
      "small-data/exhale/master_exhale14.wav\n",
      "small-data/exhale/master_exhale12.wav\n",
      "small-data/exhale/master_exhale36.wav\n",
      "small-data/exhale/master_exhale44.wav\n",
      "small-data/exhale/master_exhale7.wav\n",
      "small-data/exhale/master_exhale40.wav\n",
      "small-data/exhale/master_exhale42.wav\n",
      "small-data/exhale/master_exhale29.wav\n",
      "small-data/exhale/master_exhale13.wav\n",
      "small-data/exhale/master_exhale33.wav\n",
      "small-data/exhale/master_exhale46.wav\n",
      "small-data/exhale/master_exhale9.wav\n",
      "small-data/exhale/master_exhale32.wav\n",
      "small-data/exhale/master_exhale17.wav\n",
      "small-data/exhale/master_exhale18.wav\n",
      "small-data/exhale/master_exhale24.wav\n",
      "small-data/exhale/master_exhale50.wav\n",
      "small-data/exhale/master_exhale2.wav\n",
      "small-data/exhale/master_exhale30.wav\n",
      "small-data/exhale/master_exhale28.wav\n",
      "small-data/exhale/master_exhale8.wav\n",
      "small-data/exhale/master_exhale26.wav\n",
      "small-data/exhale/master_exhale16.wav\n",
      "small-data/exhale/master_exhale31.wav\n",
      "small-data/exhale/master_exhale27.wav\n",
      "small-data/exhale/master_exhale45.wav\n",
      "small-data/exhale/master_exhale47.wav\n",
      "small-data/exhale/master_exhale35.wav\n",
      "small-data/exhale/master_exhale38.wav\n",
      "small-data/exhale/master_exhale34.wav\n",
      "small-data/exhale/master_exhale4.wav\n",
      "small-data/exhale/master_exhale20.wav\n",
      "small-data/exhale/master_exhale48.wav\n",
      "Exhale frames size:  50\n",
      "small-data/inhale/master_inhale20.wav\n",
      "small-data/inhale/master_inhale31.wav\n",
      "small-data/inhale/master_inhale19.wav\n",
      "small-data/inhale/master_inhale39.wav\n",
      "small-data/inhale/master_inhale9.wav\n",
      "small-data/inhale/master_inhale43.wav\n",
      "small-data/inhale/master_inhale36.wav\n",
      "small-data/inhale/master_inhale42.wav\n",
      "small-data/inhale/master_inhale6.wav\n",
      "small-data/inhale/master_inhale34.wav\n",
      "small-data/inhale/master_inhale13.wav\n",
      "small-data/inhale/master_inhale33.wav\n",
      "small-data/inhale/master_inhale15.wav\n",
      "small-data/inhale/master_inhale29.wav\n",
      "small-data/inhale/master_inhale48.wav\n",
      "small-data/inhale/master_inhale12.wav\n",
      "small-data/inhale/master_inhale28.wav\n",
      "small-data/inhale/master_inhale25.wav\n",
      "small-data/inhale/master_inhale46.wav\n",
      "small-data/inhale/master_inhale14.wav\n",
      "small-data/inhale/master_inhale16.wav\n",
      "small-data/inhale/master_inhale27.wav\n",
      "small-data/inhale/master_inhale22.wav\n",
      "small-data/inhale/master_inhale11.wav\n",
      "small-data/inhale/master_inhale32.wav\n",
      "small-data/inhale/master_inhale8.wav\n",
      "small-data/inhale/master_inhale30.wav\n",
      "small-data/inhale/master_inhale24.wav\n",
      "small-data/inhale/master_inhale50.wav\n",
      "small-data/inhale/master_inhale5.wav\n",
      "small-data/inhale/master_inhale7.wav\n",
      "small-data/inhale/master_inhale21.wav\n",
      "small-data/inhale/master_inhale3.wav\n",
      "small-data/inhale/master_inhale35.wav\n",
      "small-data/inhale/master_inhale38.wav\n",
      "small-data/inhale/master_inhale26.wav\n",
      "small-data/inhale/master_inhale37.wav\n",
      "small-data/inhale/master_inhale2.wav\n",
      "small-data/inhale/master_inhale1.wav\n",
      "small-data/inhale/master_inhale47.wav\n",
      "small-data/inhale/master_inhale18.wav\n",
      "small-data/inhale/master_inhale17.wav\n",
      "small-data/inhale/master_inhale49.wav\n",
      "small-data/inhale/master_inhale41.wav\n",
      "small-data/inhale/master_inhale10.wav\n",
      "small-data/inhale/master_inhale40.wav\n",
      "small-data/inhale/master_inhale23.wav\n",
      "small-data/inhale/master_inhale4.wav\n",
      "small-data/inhale/master_inhale45.wav\n",
      "small-data/inhale/master_inhale44.wav\n",
      "Inhale frames size:  50\n",
      "small-data/silence/master_silence28.wav\n",
      "small-data/silence/master_silence22.wav\n",
      "small-data/silence/master_silence44.wav\n",
      "small-data/silence/master_silence21.wav\n",
      "small-data/silence/master_silence12.wav\n",
      "small-data/silence/master_silence35.wav\n",
      "small-data/silence/master_silence16.wav\n",
      "small-data/silence/master_silence5.wav\n",
      "small-data/silence/master_silence48.wav\n",
      "small-data/silence/master_silence38.wav\n",
      "small-data/silence/master_silence24.wav\n",
      "small-data/silence/master_silence20.wav\n",
      "small-data/silence/master_silence11.wav\n",
      "small-data/silence/master_silence41.wav\n",
      "small-data/silence/master_silence15.wav\n",
      "small-data/silence/master_silence47.wav\n",
      "small-data/silence/master_silence4.wav\n",
      "small-data/silence/master_silence17.wav\n",
      "small-data/silence/master_silence31.wav\n",
      "small-data/silence/master_silence25.wav\n",
      "small-data/silence/master_silence10.wav\n",
      "small-data/silence/master_silence6.wav\n",
      "small-data/silence/master_silence2.wav\n",
      "small-data/silence/master_silence46.wav\n",
      "small-data/silence/master_silence32.wav\n",
      "small-data/silence/master_silence43.wav\n",
      "small-data/silence/master_silence27.wav\n",
      "small-data/silence/master_silence42.wav\n",
      "small-data/silence/master_silence45.wav\n",
      "small-data/silence/master_silence3.wav\n",
      "small-data/silence/master_silence9.wav\n",
      "small-data/silence/master_silence19.wav\n",
      "small-data/silence/master_silence34.wav\n",
      "small-data/silence/master_silence37.wav\n",
      "small-data/silence/master_silence18.wav\n",
      "small-data/silence/master_silence7.wav\n",
      "small-data/silence/master_silence30.wav\n",
      "small-data/silence/master_silence33.wav\n",
      "small-data/silence/master_silence39.wav\n",
      "small-data/silence/master_silence29.wav\n",
      "small-data/silence/master_silence36.wav\n",
      "small-data/silence/master_silence8.wav\n",
      "small-data/silence/master_silence50.wav\n",
      "small-data/silence/master_silence13.wav\n",
      "small-data/silence/master_silence26.wav\n",
      "small-data/silence/master_silence40.wav\n",
      "small-data/silence/master_silence23.wav\n",
      "small-data/silence/master_silence1.wav\n",
      "small-data/silence/master_silence49.wav\n",
      "small-data/silence/master_silence14.wav\n",
      "Silence frames size:  50\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Loader**",
   "id": "9ca0c0d1656d1e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T23:10:37.600301Z",
     "start_time": "2025-01-09T23:10:37.553923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_data is a list of sequences\n",
    "# every sequence is a list of tuples (mfcc_mean, label)\n",
    "# mfcc_mean is a list of 20 MFCC coefficients\n",
    "# label is a class label (0, 1, 2)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "# Utw√≥rz instancje AudioDataset\n",
    "train_dataset = AudioDataset(train_data)\n",
    "val_dataset = AudioDataset(val_data)\n",
    "\n",
    "# Utw√≥rz DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "2351c796d1a59395",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Training**",
   "id": "7610dd8d5e792cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T23:14:55.061809Z",
     "start_time": "2025-01-09T23:10:43.738515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "REFRESH_TIME = 0.25\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE_TIME = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "\n",
    "total_time = time.time()\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Creating model...\")\n",
    "model = AudioClassifier()\n",
    "model = model.to(device)\n",
    "print(\"Model created, time: \", time.time() - start_time)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "print(\"Training model...\")\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch')\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Sp≈Çaszczenie do [batch_size * max_length, num_classes]\n",
    "        labels = labels.view(-1)  # Sp≈Çaszczenie do [batch_size * max_length]\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_accuracy += accuracy_score(labels.cpu(), predicted.cpu())\n",
    "\n",
    "        progress_bar.set_postfix(loss=running_loss / len(progress_bar),\n",
    "                                  accuracy=running_accuracy / len(progress_bar))\n",
    "    print('Train Loss: {:.4f}, Train Accuracy: {:.4f}'.format(running_loss / len(train_loader),\n",
    "                                                              running_accuracy / len(train_loader)))\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # Sp≈Çaszczenie do [batch_size * max_length, num_classes]\n",
    "            labels = labels.view(-1)  # Sp≈Çaszczenie do [batch_size * max_length]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_running_accuracy += accuracy_score(labels.cpu(), predicted.cpu())\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    avg_val_accuracy = val_running_accuracy / len(val_loader)\n",
    "    print('Val Loss: {:.4f}, Val Accuracy: {:.4f}'.format(avg_val_loss, avg_val_accuracy))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= PATIENCE_TIME:\n",
    "            print(\"Early stopping triggered. No improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "print('Finished Training, time: ', time.time() - start_time)\n",
    "print('Saving model...')\n",
    "start_time = time.time()\n",
    "torch.save(model.state_dict(), 'audio_rnn_classifier.pth')\n",
    "print(\"Model saved, time: \", time.time() - start_time)\n",
    "print(\"Finished, Total time: \", time.time() - total_time)"
   ],
   "id": "f77d53c4715f2c6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "Creating model...\n",
      "Model created, time:  0.0974419116973877\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.18s/batch, accuracy=0.832, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0425, Train Accuracy: 0.8315\n",
      "Val Loss: 1.0242, Val Accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.94s/batch, accuracy=0.813, loss=0.96] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9597, Train Accuracy: 0.8131\n",
      "Val Loss: 0.8672, Val Accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.06s/batch, accuracy=0.831, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6420, Train Accuracy: 0.8313\n",
      "Val Loss: 0.6379, Val Accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.09s/batch, accuracy=0.827, loss=0.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4863, Train Accuracy: 0.8274\n",
      "Val Loss: 0.3003, Val Accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.28s/batch, accuracy=0.822, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2805, Train Accuracy: 0.8217\n",
      "Val Loss: 0.3025, Val Accuracy: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.55s/batch, accuracy=0.845, loss=0.259] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2593, Train Accuracy: 0.8449\n",
      "Val Loss: 0.2813, Val Accuracy: 0.8373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.01s/batch, accuracy=0.895, loss=0.231] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2312, Train Accuracy: 0.8952\n",
      "Val Loss: 0.2701, Val Accuracy: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.94s/batch, accuracy=0.896, loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2353, Train Accuracy: 0.8964\n",
      "Val Loss: 0.2612, Val Accuracy: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.33s/batch, accuracy=0.9, loss=0.215]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2151, Train Accuracy: 0.9002\n",
      "Val Loss: 0.2546, Val Accuracy: 0.8407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.27s/batch, accuracy=0.906, loss=0.199] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1990, Train Accuracy: 0.9057\n",
      "Val Loss: 0.2479, Val Accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.46s/batch, accuracy=0.909, loss=0.191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1907, Train Accuracy: 0.9086\n",
      "Val Loss: 0.2460, Val Accuracy: 0.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.25s/batch, accuracy=0.911, loss=0.185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1847, Train Accuracy: 0.9109\n",
      "Val Loss: 0.2706, Val Accuracy: 0.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.43s/batch, accuracy=0.907, loss=0.187] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1875, Train Accuracy: 0.9067\n",
      "Val Loss: 0.2941, Val Accuracy: 0.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.21s/batch, accuracy=0.905, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1791, Train Accuracy: 0.9053\n",
      "Val Loss: 0.2999, Val Accuracy: 0.8303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.11s/batch, accuracy=0.948, loss=0.165] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1653, Train Accuracy: 0.9480\n",
      "Val Loss: 0.2963, Val Accuracy: 0.9587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.13s/batch, accuracy=0.972, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1714, Train Accuracy: 0.9717\n",
      "Val Loss: 0.2924, Val Accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.41s/batch, accuracy=0.975, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1547, Train Accuracy: 0.9748\n",
      "Val Loss: 0.2868, Val Accuracy: 0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.31s/batch, accuracy=0.979, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1607, Train Accuracy: 0.9788\n",
      "Val Loss: 0.2809, Val Accuracy: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.51s/batch, accuracy=0.981, loss=0.147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1468, Train Accuracy: 0.9812\n",
      "Val Loss: 0.2748, Val Accuracy: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.41s/batch, accuracy=0.983, loss=0.155] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1549, Train Accuracy: 0.9830\n",
      "Val Loss: 0.2677, Val Accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.45s/batch, accuracy=0.985, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1303, Train Accuracy: 0.9855\n",
      "Val Loss: 0.2642, Val Accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.49s/batch, accuracy=0.985, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1340, Train Accuracy: 0.9849\n",
      "Val Loss: 0.2605, Val Accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.39s/batch, accuracy=0.987, loss=0.127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1272, Train Accuracy: 0.9871\n",
      "Val Loss: 0.2566, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.41s/batch, accuracy=0.986, loss=0.134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1337, Train Accuracy: 0.9861\n",
      "Val Loss: 0.2528, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.35s/batch, accuracy=0.986, loss=0.132] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1320, Train Accuracy: 0.9865\n",
      "Val Loss: 0.2493, Val Accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.57s/batch, accuracy=0.988, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1183, Train Accuracy: 0.9881\n",
      "Val Loss: 0.2474, Val Accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.50s/batch, accuracy=0.987, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1177, Train Accuracy: 0.9874\n",
      "Val Loss: 0.2452, Val Accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.26s/batch, accuracy=0.987, loss=0.119] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1187, Train Accuracy: 0.9873\n",
      "Val Loss: 0.2432, Val Accuracy: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.61s/batch, accuracy=0.988, loss=0.114] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1139, Train Accuracy: 0.9878\n",
      "Val Loss: 0.2413, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.33s/batch, accuracy=0.986, loss=0.118] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1179, Train Accuracy: 0.9863\n",
      "Val Loss: 0.2393, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.43s/batch, accuracy=0.988, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1100, Train Accuracy: 0.9884\n",
      "Val Loss: 0.2383, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.47s/batch, accuracy=0.988, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1236, Train Accuracy: 0.9883\n",
      "Val Loss: 0.2371, Val Accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:06<00:00,  2.20s/batch, accuracy=0.99, loss=0.105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1052, Train Accuracy: 0.9903\n",
      "Val Loss: 0.2360, Val Accuracy: 0.9740\n",
      "Early stopping triggered. No improvement in validation accuracy.\n",
      "Finished Training, time:  251.04413747787476\n",
      "Saving model...\n",
      "Model saved, time:  0.05625510215759277\n",
      "Finished, Total time:  251.2057912349701\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4920c770ed8d9224"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
