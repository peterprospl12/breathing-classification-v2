{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data preprocessing**",
   "id": "f60c5ddf4c188baa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:52:57.895930Z",
     "start_time": "2025-01-25T20:52:50.247991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "REFRESH_TIME = 0.25  # seconds\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Directories with data\n",
    "exhale_dir = 'small-data/exhale'\n",
    "inhale_dir = 'small-data/inhale'\n",
    "silence_dir = 'small-data/silence'\n",
    "\n",
    "# Creating list of files\n",
    "exhale_files = [os.path.join(exhale_dir, file) for file in os.listdir(exhale_dir)]\n",
    "inhale_files = [os.path.join(inhale_dir, file) for file in os.listdir(inhale_dir)]\n",
    "silence_files = [os.path.join(silence_dir, file) for file in os.listdir(silence_dir)]\n",
    "train_data = []\n",
    "files_list = [exhale_files, inhale_files, silence_files]\n",
    "files_names = ['exhale', 'inhale', 'silence']\n",
    "\n",
    "# Amount of sequences of every class\n",
    "exhale_frames_size = 0\n",
    "inhale_frames_size = 0\n",
    "silence_frames_size = 0\n",
    "\n",
    "# Main loop to preprocess data into MFCCs\n",
    "for label, files in enumerate(files_list):\n",
    "\n",
    "    # Iterate through all files (potentially longer audio recording with different classes)\n",
    "    for file in files:  # file - wav file path\n",
    "\n",
    "        # Load vaw file and keep file's sampling rate\n",
    "        y, sr = librosa.load(file, mono=True)  # y - frames, sr - wav file's sampling rate\n",
    "\n",
    "        # Calculate chunk size\n",
    "        chunk_size = int(sr * 0.25)  # for example 48000 * 0.25 = 12000 frames per chunk\n",
    "\n",
    "        # List of MFCCs for every data sequence (it will be a list of lists of tuples (mfcc coefficients, label))\n",
    "        mfcc_sequence = []\n",
    "\n",
    "        # Iterate trough every 0.25s audio chunk\n",
    "        for i in range(0, len(y), chunk_size):\n",
    "\n",
    "            # Get frames of current chunk\n",
    "            frame = y[i:i + chunk_size]  # list of frames\n",
    "\n",
    "            if len(frame) == chunk_size:  # Ignore the last frame if it's shorter\n",
    "\n",
    "                # Calculate MFCCs (it will be a vector of MFCC coefficients - a vector of vectors)\n",
    "                mfcc = librosa.feature.mfcc(y=frame, sr=sr)  # Default n_mfcc = 20 (20 coefficients per subframe)\n",
    "\n",
    "                # Because we have a list of MFCC vectors, we can calculate the mean of every coefficient so we get just one set of coefficients for every 0.25s chunk\n",
    "                mfcc_mean = mfcc.mean(axis=1)  # list of 20 MFCC coefficients\n",
    "\n",
    "                # Append the mean of MFCCs to the list of MFCCs for the current data sequence\n",
    "                mfcc_sequence.append((mfcc_mean, label))\n",
    "\n",
    "        print(file)\n",
    "\n",
    "        if mfcc_sequence:\n",
    "\n",
    "            # Append the list of MFCCs for the current data sequence to the list of all data sequences\n",
    "            train_data.append(mfcc_sequence)\n",
    "\n",
    "    # Print the amount of sequences for every class\n",
    "    if label == 0:\n",
    "        exhale_frames_size = len(train_data)\n",
    "        print(\"Exhale frames size: \", exhale_frames_size)\n",
    "    elif label == 1:\n",
    "        inhale_frames_size = len(train_data) - exhale_frames_size\n",
    "        print(\"Inhale frames size: \", inhale_frames_size)\n",
    "    else:\n",
    "        silence_frames_size = len(train_data) - exhale_frames_size - inhale_frames_size\n",
    "        print(\"Silence frames size: \", silence_frames_size)"
   ],
   "id": "2e259d6856399c02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small-data/exhale/master_exhale3.wav\n",
      "small-data/exhale/master_exhale6.wav\n",
      "small-data/exhale/master_exhale5.wav\n",
      "small-data/exhale/master_exhale15.wav\n",
      "small-data/exhale/master_exhale37.wav\n",
      "small-data/exhale/master_exhale21.wav\n",
      "small-data/exhale/master_exhale41.wav\n",
      "small-data/exhale/master_exhale25.wav\n",
      "small-data/exhale/master_exhale43.wav\n",
      "small-data/exhale/master_exhale39.wav\n",
      "small-data/exhale/master_exhale1.wav\n",
      "small-data/exhale/master_exhale22.wav\n",
      "small-data/exhale/master_exhale11.wav\n",
      "small-data/exhale/master_exhale49.wav\n",
      "small-data/exhale/master_exhale19.wav\n",
      "small-data/exhale/master_exhale23.wav\n",
      "small-data/exhale/master_exhale10.wav\n",
      "small-data/exhale/master_exhale14.wav\n",
      "small-data/exhale/master_exhale12.wav\n",
      "small-data/exhale/master_exhale36.wav\n",
      "small-data/exhale/master_exhale44.wav\n",
      "small-data/exhale/master_exhale7.wav\n",
      "small-data/exhale/master_exhale40.wav\n",
      "small-data/exhale/master_exhale42.wav\n",
      "small-data/exhale/master_exhale29.wav\n",
      "small-data/exhale/master_exhale13.wav\n",
      "small-data/exhale/master_exhale33.wav\n",
      "small-data/exhale/master_exhale46.wav\n",
      "small-data/exhale/master_exhale9.wav\n",
      "small-data/exhale/master_exhale32.wav\n",
      "small-data/exhale/master_exhale17.wav\n",
      "small-data/exhale/master_exhale18.wav\n",
      "small-data/exhale/master_exhale24.wav\n",
      "small-data/exhale/master_exhale50.wav\n",
      "small-data/exhale/master_exhale2.wav\n",
      "small-data/exhale/master_exhale30.wav\n",
      "small-data/exhale/master_exhale28.wav\n",
      "small-data/exhale/master_exhale8.wav\n",
      "small-data/exhale/master_exhale26.wav\n",
      "small-data/exhale/master_exhale16.wav\n",
      "small-data/exhale/master_exhale31.wav\n",
      "small-data/exhale/master_exhale27.wav\n",
      "small-data/exhale/master_exhale45.wav\n",
      "small-data/exhale/master_exhale47.wav\n",
      "small-data/exhale/master_exhale35.wav\n",
      "small-data/exhale/master_exhale38.wav\n",
      "small-data/exhale/master_exhale34.wav\n",
      "small-data/exhale/master_exhale4.wav\n",
      "small-data/exhale/master_exhale20.wav\n",
      "small-data/exhale/master_exhale48.wav\n",
      "Exhale frames size:  50\n",
      "small-data/inhale/master_inhale20.wav\n",
      "small-data/inhale/master_inhale31.wav\n",
      "small-data/inhale/master_inhale19.wav\n",
      "small-data/inhale/master_inhale39.wav\n",
      "small-data/inhale/master_inhale9.wav\n",
      "small-data/inhale/master_inhale43.wav\n",
      "small-data/inhale/master_inhale36.wav\n",
      "small-data/inhale/master_inhale42.wav\n",
      "small-data/inhale/master_inhale6.wav\n",
      "small-data/inhale/master_inhale34.wav\n",
      "small-data/inhale/master_inhale13.wav\n",
      "small-data/inhale/master_inhale33.wav\n",
      "small-data/inhale/master_inhale15.wav\n",
      "small-data/inhale/master_inhale29.wav\n",
      "small-data/inhale/master_inhale48.wav\n",
      "small-data/inhale/master_inhale12.wav\n",
      "small-data/inhale/master_inhale28.wav\n",
      "small-data/inhale/master_inhale25.wav\n",
      "small-data/inhale/master_inhale46.wav\n",
      "small-data/inhale/master_inhale14.wav\n",
      "small-data/inhale/master_inhale16.wav\n",
      "small-data/inhale/master_inhale27.wav\n",
      "small-data/inhale/master_inhale22.wav\n",
      "small-data/inhale/master_inhale11.wav\n",
      "small-data/inhale/master_inhale32.wav\n",
      "small-data/inhale/master_inhale8.wav\n",
      "small-data/inhale/master_inhale30.wav\n",
      "small-data/inhale/master_inhale24.wav\n",
      "small-data/inhale/master_inhale50.wav\n",
      "small-data/inhale/master_inhale5.wav\n",
      "small-data/inhale/master_inhale7.wav\n",
      "small-data/inhale/master_inhale21.wav\n",
      "small-data/inhale/master_inhale3.wav\n",
      "small-data/inhale/master_inhale35.wav\n",
      "small-data/inhale/master_inhale38.wav\n",
      "small-data/inhale/master_inhale26.wav\n",
      "small-data/inhale/master_inhale37.wav\n",
      "small-data/inhale/master_inhale2.wav\n",
      "small-data/inhale/master_inhale1.wav\n",
      "small-data/inhale/master_inhale47.wav\n",
      "small-data/inhale/master_inhale18.wav\n",
      "small-data/inhale/master_inhale17.wav\n",
      "small-data/inhale/master_inhale49.wav\n",
      "small-data/inhale/master_inhale41.wav\n",
      "small-data/inhale/master_inhale10.wav\n",
      "small-data/inhale/master_inhale40.wav\n",
      "small-data/inhale/master_inhale23.wav\n",
      "small-data/inhale/master_inhale4.wav\n",
      "small-data/inhale/master_inhale45.wav\n",
      "small-data/inhale/master_inhale44.wav\n",
      "Inhale frames size:  50\n",
      "small-data/silence/master_silence28.wav\n",
      "small-data/silence/master_silence22.wav\n",
      "small-data/silence/master_silence44.wav\n",
      "small-data/silence/master_silence21.wav\n",
      "small-data/silence/master_silence12.wav\n",
      "small-data/silence/master_silence35.wav\n",
      "small-data/silence/master_silence16.wav\n",
      "small-data/silence/master_silence5.wav\n",
      "small-data/silence/master_silence48.wav\n",
      "small-data/silence/master_silence38.wav\n",
      "small-data/silence/master_silence24.wav\n",
      "small-data/silence/master_silence20.wav\n",
      "small-data/silence/master_silence11.wav\n",
      "small-data/silence/master_silence41.wav\n",
      "small-data/silence/master_silence15.wav\n",
      "small-data/silence/master_silence47.wav\n",
      "small-data/silence/master_silence4.wav\n",
      "small-data/silence/master_silence17.wav\n",
      "small-data/silence/master_silence31.wav\n",
      "small-data/silence/master_silence25.wav\n",
      "small-data/silence/master_silence10.wav\n",
      "small-data/silence/master_silence6.wav\n",
      "small-data/silence/master_silence2.wav\n",
      "small-data/silence/master_silence46.wav\n",
      "small-data/silence/master_silence32.wav\n",
      "small-data/silence/master_silence43.wav\n",
      "small-data/silence/master_silence27.wav\n",
      "small-data/silence/master_silence42.wav\n",
      "small-data/silence/master_silence45.wav\n",
      "small-data/silence/master_silence3.wav\n",
      "small-data/silence/master_silence9.wav\n",
      "small-data/silence/master_silence19.wav\n",
      "small-data/silence/master_silence34.wav\n",
      "small-data/silence/master_silence37.wav\n",
      "small-data/silence/master_silence18.wav\n",
      "small-data/silence/master_silence7.wav\n",
      "small-data/silence/master_silence30.wav\n",
      "small-data/silence/master_silence33.wav\n",
      "small-data/silence/master_silence39.wav\n",
      "small-data/silence/master_silence29.wav\n",
      "small-data/silence/master_silence36.wav\n",
      "small-data/silence/master_silence8.wav\n",
      "small-data/silence/master_silence50.wav\n",
      "small-data/silence/master_silence13.wav\n",
      "small-data/silence/master_silence26.wav\n",
      "small-data/silence/master_silence40.wav\n",
      "small-data/silence/master_silence23.wav\n",
      "small-data/silence/master_silence1.wav\n",
      "small-data/silence/master_silence49.wav\n",
      "small-data/silence/master_silence14.wav\n",
      "Silence frames size:  50\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Data Loader**",
   "id": "9ca0c0d1656d1e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:53:06.484387Z",
     "start_time": "2025-01-25T20:53:06.476258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_classes import AudioDataset\n",
    "import torch\n",
    "\n",
    "# train_data is a list of sequences\n",
    "# every sequence is a list of tuples (mfcc_mean, label)\n",
    "# mfcc_mean is a list of 20 MFCC coefficients\n",
    "# label is a class label (0, 1, 2)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "# We need to create a DataLoader object for training and validation sets\n",
    "train_dataset = AudioDataset(train_data)\n",
    "val_dataset = AudioDataset(val_data)\n",
    "\n",
    "# Padding for sequence (necessary for DataLoader)\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # Get the sequences and labels from the batch\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # Get the lengths of sequences\n",
    "    lengths = [seq.size(0) for seq in sequences]\n",
    "\n",
    "    # Get the maximum length\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    # Create padded sequences and labels\n",
    "    padded_sequences = torch.zeros(len(sequences), max_length, 20)\n",
    "    padded_labels = torch.zeros(len(sequences), max_length, dtype=torch.long)\n",
    "\n",
    "    # Fill the padded sequences and labels\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :seq.size(0), :] = seq\n",
    "        padded_labels[i, :len(labels[i])] = labels[i]\n",
    "\n",
    "    # Return padded sequences and labels\n",
    "    return padded_sequences, padded_labels\n",
    "\n",
    "# And then we can create DataLoader objects, that we can use in training\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "2351c796d1a59395",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Training**",
   "id": "7610dd8d5e792cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:53:12.155917Z",
     "start_time": "2025-01-25T20:53:10.341573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from model_classes import AudioClassifierLSTM as AudioClassifier\n",
    "import torch.nn as nn\n",
    "\n",
    "REFRESH_TIME = 0.25  # Refresh time in seconds in future realtime\n",
    "NUM_EPOCHS = 100  # Number of epochs (the more epoch the better model, but it takes more time)\n",
    "PATIENCE_TIME = 10  # Number of epochs without improvement in validation accuracy that will stop training\n",
    "LEARNING_RATE = 0.001  # Learning rate\n",
    "BATCH_SIZE = 32  # Batch size (amount of sequences in one batch)\n",
    "\n",
    "# Check if CUDA is available (learning on GPU is much faster)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "\n",
    "total_time = time.time()\n",
    "start_time = time.time()\n",
    "\n",
    "# Create model object\n",
    "print(\"Creating model...\")\n",
    "model = AudioClassifier()\n",
    "model = model.to(device)\n",
    "print(\"Model created, time: \", time.time() - start_time)\n",
    "\n",
    "# Define loss function and optimizer (network parameters)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# These are just for early stopping\n",
    "best_val_accuracy = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "print(\"Training model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Enable training on model object\n",
    "    model.train()\n",
    "\n",
    "    # Initialize running loss and accuracy\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    # It's just a fancy progress bar in console\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch')\n",
    "\n",
    "    # Iterate through batches\n",
    "    for inputs, labels in progress_bar:\n",
    "\n",
    "        # Move inputs and labels to the device (GPU or CPU)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Flattening outputs and labels from [batch_size, max_length, num_classes]\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # Flattening to [batch_size * max_length, num_classes]\n",
    "        labels = labels.view(-1)  # Flattening to [batch_size * max_length]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass (calculate gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights according to the calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate running loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        running_accuracy += accuracy_score(labels.cpu(), predicted.cpu())\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=running_loss / len(progress_bar),\n",
    "                                  accuracy=running_accuracy / len(progress_bar))\n",
    "\n",
    "    # Print the loss and accuracy for the epoch\n",
    "    print('Train Loss: {:.4f}, Train Accuracy: {:.4f}'.format(running_loss / len(train_loader),\n",
    "                                                              running_accuracy / len(train_loader)))\n",
    "\n",
    "    # After training on the whole training set, we can evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_accuracy = 0.0\n",
    "\n",
    "    # We don't need to calculate gradients during validation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Iterate through validation set\n",
    "        for inputs, labels in val_loader:\n",
    "\n",
    "            # Move inputs and labels to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # As previous, we need to flatten outputs and labels\n",
    "            outputs = outputs.view(-1, outputs.size(-1)) # Flattening to [batch_size * max_length, num_classes]\n",
    "            labels = labels.view(-1) # Flattening to [batch_size * max_length]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate running loss (cumulative loss over batches) and add current epoch's accuracy to the running (cumulative) accuracy\n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_running_accuracy += accuracy_score(labels.cpu(), predicted.cpu())\n",
    "\n",
    "    # Calculate cumulative loss and accuracy for the validation set\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    avg_val_accuracy = val_running_accuracy / len(val_loader)\n",
    "\n",
    "    # And print it\n",
    "    print('Val Loss: {:.4f}, Val Accuracy: {:.4f}'.format(avg_val_loss, avg_val_accuracy))\n",
    "\n",
    "    # Learning rate scheduler (changing learning rate during training)\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping (if there is no improvement in validation accuracy for PATIENCE_TIME epochs, we stop training)\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= PATIENCE_TIME:\n",
    "            print(\"Early stopping triggered. No improvement in validation accuracy.\")\n",
    "            break\n",
    "\n",
    "# And print final results\n",
    "print('Finished Training, time: ', time.time() - start_time)\n",
    "print('Saving model...')\n",
    "start_time = time.time()\n",
    "#TODO\n",
    "torch.save(model.state_dict(), 'audio_rnn_classifier.pth')\n",
    "print(\"Model saved, time: \", time.time() - start_time)\n",
    "print(\"Finished, Total time: \", time.time() - total_time)"
   ],
   "id": "f77d53c4715f2c6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n",
      "Creating model...\n",
      "Model created, time:  0.016856670379638672\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/3 [00:00<?, ?batch/s]/home/tomasz/Pulpit/breathing-classification-v2/model/model_classes.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  mfcc_sequence = torch.tensor(mfcc_sequence, dtype=torch.float32)\n",
      "Epoch 1/100:   0%|          | 0/3 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 65\u001B[0m\n\u001B[1;32m     62\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# Flattening outputs and labels from [batch_size, max_length, num_classes]\u001B[39;00m\n\u001B[0;32m---> 65\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43moutputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, outputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))  \u001B[38;5;66;03m# Flattening to [batch_size * max_length, num_classes]\u001B[39;00m\n\u001B[1;32m     66\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# Flattening to [batch_size * max_length]\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'view'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4920c770ed8d9224"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
