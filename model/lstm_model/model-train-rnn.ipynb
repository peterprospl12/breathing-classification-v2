{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Data preprocessing and creating data loaders**\n",
    "Preprocessing is separated from training in this notebook to let us experiment with network hiperparameters without the need to preprocess data every time (preprocessing all dataset takes couple minutes)."
   ],
   "id": "e034c3024ed6f537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:05:26.792194Z",
     "start_time": "2025-02-11T21:05:04.316531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This script preprocess the data into MFCCs and labels. It also creates a DataLoader object for training and validation sets.\n",
    "\n",
    "The script divides sequences into 0.25s chunks and calculates MFCCs for every chunk. Then, it assigns a label to every chunk based on the labels from the CSV file. If a chunk has both sample from two different classes, the label is assigned based on the majority of samples in the chunk.\n",
    "\n",
    "The goal is to create a dataset, and final shape of output is a list of sequences. Every sequence is a list of tuples (MFCCs, label). The DataLoader object will be used to iterate through the dataset during training.\n",
    "\n",
    "Most important parameters of this script is:\n",
    "REFRESH_TIME - length of one classification window in seconds\n",
    "BATCH_SIZE - batch size for DataLoader\n",
    "data_dir - directory with training and validation data (there must be sequences in directories, ideally created with create_sequences.py script)\n",
    "\"\"\"\n",
    "from model_classes import AudioDataset\n",
    "import torch\n",
    "import os\n",
    "from scipy.io.wavfile import read\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "REFRESH_TIME = 0.25  # seconds\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Directories with data\n",
    "data_dir = '../../sequences'\n",
    "\n",
    "# Function to load labels from csv file to list of tuples (label, start_frame, end_frame)\n",
    "def load_labels(csv_file_v):\n",
    "    labels_v = []\n",
    "    with open(csv_file_v, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header\n",
    "        for row in reader:\n",
    "            if row[0] == 'silence':\n",
    "                labels_v.append((2, int(row[1]), int(row[2])))  # 2: silence\n",
    "            elif row[0] == 'inhale':\n",
    "                labels_v.append((1, int(row[1]), int(row[2])))  # 1: inhale\n",
    "            elif row[0] == 'exhale':\n",
    "                labels_v.append((0, int(row[1]), int(row[2])))  # 0: exhale\n",
    "    return labels_v\n",
    "\n",
    "# Function to get the label for a given part of recording (from start_frame to end_frame)\n",
    "def get_label_for_time(labels_v, start_frame, end_frame):\n",
    "    label_counts = [0, 0, 0]  # 0: exhale, 1: inhale, 2: silence\n",
    "\n",
    "    for label_it, start, end in labels_v:\n",
    "        if start < end_frame and end > start_frame:\n",
    "            overlap_start = max(start, start_frame)\n",
    "            overlap_end = min(end, end_frame)\n",
    "            overlap_length = overlap_end - overlap_start\n",
    "            label_counts[label_it] += overlap_length\n",
    "\n",
    "    return label_counts.index(max(label_counts))\n",
    "\n",
    "# Creating list of files\n",
    "wav_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.wav')]\n",
    "train_data = []\n",
    "\n",
    "# Main loop to preprocess data into MFCCs\n",
    "for wav_file in wav_files:\n",
    "    csv_file = wav_file.replace('.wav', '.csv')\n",
    "\n",
    "    # Ensure that there is a corresponding CSV file\n",
    "    if not os.path.exists(csv_file):\n",
    "        continue\n",
    "\n",
    "    # Load audio and labels\n",
    "    sr, y = read(wav_file)\n",
    "\n",
    "    # Throw error if sampling rate is not 44100, recording is not in mono or dtype is not int16\n",
    "    if sr != 44100:\n",
    "        raise Exception(\"Sampling rate is not 44100. Make sure you have used right sequence creator.\")\n",
    "    if y.dtype != np.int16:\n",
    "        raise Exception(\"Data type is not int16. Make sure you have used right sequence creator.\")\n",
    "    if y.ndim != 1:\n",
    "        raise Exception(\"Audio is not mono. Make sure you have used right sequence creator.\")\n",
    "\n",
    "    # Load labels from CSV file\n",
    "    labels = load_labels(csv_file)\n",
    "\n",
    "    # Calculate chunk size\n",
    "    chunk_size = int(sr * REFRESH_TIME)\n",
    "\n",
    "    # List of MFCCs for every data sequence (it will be a list of lists of tuples (mfcc coefficients, label))\n",
    "    mfcc_sequence = []\n",
    "\n",
    "    # Iterate through every 0.25s audio chunk\n",
    "    for i in range(0, len(y), chunk_size):\n",
    "        # Get frame's samples\n",
    "        frame = y[i:i + chunk_size]\n",
    "\n",
    "        # Ensure that the frame has the right size\n",
    "        if len(frame) == chunk_size:\n",
    "            # Conversion to float32 from int16\n",
    "            if frame.dtype != np.int16:\n",
    "                raise Exception(\"Data type is not int16. Make sure you have used right sequence creator.\")\n",
    "            frames_float32 = frame.astype(np.float32) / np.iinfo(np.int16).max\n",
    "\n",
    "            # Make sure that frame is mono, 44100 Hz and converted to float32\n",
    "            if frames_float32.ndim != 1:\n",
    "                raise Exception(\"Audio is not mono. Make sure you have used right sequence creator.\")\n",
    "            if frames_float32.dtype != np.float32:\n",
    "                raise Exception(\"Data type is not float32. Make sure you have used right sequence creator.\")\n",
    "            if sr != 44100:\n",
    "                raise Exception(\"Sampling rate is not 44100. Make sure you have used right sequence creator.\")\n",
    "\n",
    "            # Calculate MFCCs\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                y=frames_float32,\n",
    "                sr=sr,\n",
    "                n_mfcc=20,\n",
    "                n_mels=40,\n",
    "                fmin=20,\n",
    "                fmax=8000,\n",
    "                lifter=22,\n",
    "                norm=\"ortho\"\n",
    "            )\n",
    "\n",
    "            # Because function will return x times 13 MFCCs, we will calculate mean of them (size of mfcc above is [13, x])\n",
    "            mfcc_mean = mfcc.mean(axis=1)\n",
    "\n",
    "            # Get label for the frame\n",
    "            label = get_label_for_time(labels, i, i + chunk_size)\n",
    "\n",
    "            # Append MFCCs and label to the sequence (we append tuple of a ndarray of length 13 and a label)\n",
    "            mfcc_sequence.append((mfcc_mean, label))\n",
    "\n",
    "    train_data.append(mfcc_sequence)  # Append sequence to the list of sequences\n",
    "\n",
    "# Ensure that all sequences have the same length\n",
    "length = len(train_data[0])\n",
    "for sequence in train_data:\n",
    "    if len(sequence) != length:\n",
    "        raise Exception(\"Sequences have different lengths\")\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_dataset = AudioDataset(train_data)\n",
    "val_dataset = AudioDataset(val_data)\n",
    "\n",
    "# DataLoader and collate function (collate function is used to pad sequences to the same length, but our sequences should have the same length)\n",
    "def collate_fn(batch):\n",
    "    sequences, labels_t = zip(*batch)\n",
    "    lengths_t = [seq.size(0) for seq in sequences]\n",
    "    max_length = max(lengths_t)\n",
    "    padded_sequences = torch.zeros(len(sequences), max_length, 20)\n",
    "    padded_labels = torch.zeros(len(sequences), max_length, dtype=torch.long)\n",
    "    for j, seq in enumerate(sequences):\n",
    "        padded_sequences[j, :seq.size(0), :] = seq\n",
    "        padded_labels[j, :len(labels_t[j])] = labels_t[j]\n",
    "    return padded_sequences, padded_labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ],
   "id": "2e259d6856399c02",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Training**",
   "id": "7610dd8d5e792cad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T21:14:44.926731Z",
     "start_time": "2025-02-11T21:14:11.445653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from model_classes import AudioClassifierLSTM\n",
    "\n",
    "architecture = 'LSTM'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE_TIME = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "\n",
    "model = AudioClassifierLSTM().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "val_loss_on_best_val_acc = 0.0\n",
    "train_loss_on_best_val_acc = 0.0\n",
    "train_acc_on_best_val_acc = 0.0\n",
    "\n",
    "early_stopping_counter = 0\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)  # Shape: [batch, time_steps, features]\n",
    "        labels = labels.to(device)  # Shape: [batch, time_steps]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs)  # outputs.shape: [batch, time_steps, num_classes]\n",
    "\n",
    "        # Flattening to [batch * time_steps, num_classes]\n",
    "        outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "        labels_flat = labels.view(-1)  # [batch * time_steps]\n",
    "\n",
    "        loss = criterion(outputs_flat, labels_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate loss and accuracy\n",
    "        _, predicted = torch.max(outputs_flat, 1)  # Get the predicted class (index of the maximum logit) for each audio segment\n",
    "        train_correct += (predicted == labels_flat).sum().item()  # Count how many predictions match the true labels in this batch\n",
    "        train_total += labels_flat.size(0)  # Update the total number of audio segments processed so far\n",
    "        train_loss += loss.item()  # Accumulate the loss for this batch to calculate the average loss later\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Switch the model to evaluation mode (turns off dropout, batch norm, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to track total correct predictions, total samples, and accumulated loss for validation\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "    # Disable gradient calculation for validation (saves memory and speeds up computation)\n",
    "    with torch.no_grad():\n",
    "        # Iterate through batches in the validation set\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)  # Shape: [batch, time_steps, features]\n",
    "            labels = labels.to(device)  # Shape: [batch, time_steps]\n",
    "\n",
    "            # Forward pass: compute model predictions\n",
    "            outputs, _ = model(inputs)  # outputs.shape: [batch, time_steps, num_classes]\n",
    "\n",
    "            # Flattening to [batch * time_steps, num_classes]\n",
    "            outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "            labels_flat = labels.view(-1)  # [batch * time_steps]\n",
    "\n",
    "            # Calculate loss (how far the model's predictions are from the correct answers)\n",
    "            loss = criterion(outputs_flat, labels_flat)\n",
    "            val_loss += loss.item()  # Accumulate the loss to calculate the average loss later\n",
    "\n",
    "            # Calculate accuracy for this batch\n",
    "            _, predicted = torch.max(outputs_flat, 1)  # Get the predicted class (index of the maximum logit) for each audio segment\n",
    "            val_correct += (predicted == labels_flat).sum().item()  # Count how many predictions match the true labels in this batch\n",
    "            val_total += labels_flat.size(0)  # Update the total number of audio segments processed so far\n",
    "\n",
    "    # Calculate the average validation loss and accuracy for the entire epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Average loss = total loss / number of batches\n",
    "    val_acc = val_correct / val_total  # Accuracy = correct predictions / total samples\n",
    "\n",
    "\n",
    "    # Update and early stopping\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        train_acc_on_best_val_acc = train_acc\n",
    "        val_loss_on_best_val_acc = avg_val_loss\n",
    "        train_loss_on_best_val_acc = avg_train_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(model.state_dict(), f'best_model_{architecture}_{best_val_accuracy:.4f}.pth')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS} [{epoch_time:.2f}s]')\n",
    "    print(f' Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}')\n",
    "    print(f' Val Loss: {avg_val_loss:.4f}, Acc: {val_acc:.4f}\\n')\n",
    "\n",
    "    if early_stopping_counter >= PATIENCE_TIME:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Save the final model after training completes\n",
    "\n",
    "print(f\"Final model saved to lstm_model_{architecture}_{best_val_accuracy}.pth\")\n",
    "\n",
    "# Print metrics for the final model\n",
    "print(\"\\nFinal Model Metrics:\")\n",
    "print(f' Train Loss: {train_loss_on_best_val_acc:.4f}, Train Acc: {train_acc_on_best_val_acc:.4f}')\n",
    "print(f' Val Loss: {val_loss_on_best_val_acc:.4f}, Val Acc: {best_val_accuracy:.4f}')\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f'Total training time: {total_time:.2f}s')"
   ],
   "id": "f77d53c4715f2c6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomasz/Pulpit/breathing-classification-v2/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 [1.85s]\n",
      " Train Loss: 1.0489, Acc: 0.5759\n",
      " Val Loss: 0.9437, Acc: 0.8239\n",
      "\n",
      "Epoch 2/100 [1.51s]\n",
      " Train Loss: 0.7912, Acc: 0.8032\n",
      " Val Loss: 0.5791, Acc: 0.8415\n",
      "\n",
      "Epoch 3/100 [1.73s]\n",
      " Train Loss: 0.5055, Acc: 0.8272\n",
      " Val Loss: 0.4210, Acc: 0.8426\n",
      "\n",
      "Epoch 4/100 [1.28s]\n",
      " Train Loss: 0.4192, Acc: 0.8421\n",
      " Val Loss: 0.3797, Acc: 0.8489\n",
      "\n",
      "Epoch 5/100 [1.27s]\n",
      " Train Loss: 0.3662, Acc: 0.8645\n",
      " Val Loss: 0.3833, Acc: 0.8506\n",
      "\n",
      "Epoch 6/100 [1.15s]\n",
      " Train Loss: 0.3366, Acc: 0.8809\n",
      " Val Loss: 0.3535, Acc: 0.8528\n",
      "\n",
      "Epoch 7/100 [1.22s]\n",
      " Train Loss: 0.3045, Acc: 0.8932\n",
      " Val Loss: 0.3603, Acc: 0.8682\n",
      "\n",
      "Epoch 8/100 [1.13s]\n",
      " Train Loss: 0.2925, Acc: 0.8947\n",
      " Val Loss: 0.3487, Acc: 0.8716\n",
      "\n",
      "Epoch 9/100 [1.10s]\n",
      " Train Loss: 0.2783, Acc: 0.9016\n",
      " Val Loss: 0.3316, Acc: 0.8733\n",
      "\n",
      "Epoch 10/100 [1.10s]\n",
      " Train Loss: 0.2612, Acc: 0.9114\n",
      " Val Loss: 0.3294, Acc: 0.8744\n",
      "\n",
      "Epoch 11/100 [1.09s]\n",
      " Train Loss: 0.2424, Acc: 0.9201\n",
      " Val Loss: 0.3154, Acc: 0.8727\n",
      "\n",
      "Epoch 12/100 [1.06s]\n",
      " Train Loss: 0.2324, Acc: 0.9262\n",
      " Val Loss: 0.3215, Acc: 0.8756\n",
      "\n",
      "Epoch 13/100 [1.06s]\n",
      " Train Loss: 0.2238, Acc: 0.9279\n",
      " Val Loss: 0.3201, Acc: 0.8773\n",
      "\n",
      "Epoch 14/100 [1.05s]\n",
      " Train Loss: 0.2176, Acc: 0.9299\n",
      " Val Loss: 0.3249, Acc: 0.8773\n",
      "\n",
      "Epoch 15/100 [1.04s]\n",
      " Train Loss: 0.2109, Acc: 0.9315\n",
      " Val Loss: 0.3234, Acc: 0.8773\n",
      "\n",
      "Epoch 16/100 [0.97s]\n",
      " Train Loss: 0.2046, Acc: 0.9355\n",
      " Val Loss: 0.3242, Acc: 0.8761\n",
      "\n",
      "Epoch 17/100 [0.96s]\n",
      " Train Loss: 0.1995, Acc: 0.9396\n",
      " Val Loss: 0.3235, Acc: 0.8750\n",
      "\n",
      "Epoch 18/100 [0.96s]\n",
      " Train Loss: 0.1944, Acc: 0.9396\n",
      " Val Loss: 0.3227, Acc: 0.8756\n",
      "\n",
      "Epoch 19/100 [1.03s]\n",
      " Train Loss: 0.1917, Acc: 0.9396\n",
      " Val Loss: 0.3218, Acc: 0.8761\n",
      "\n",
      "Epoch 20/100 [1.21s]\n",
      " Train Loss: 0.1876, Acc: 0.9416\n",
      " Val Loss: 0.3242, Acc: 0.8773\n",
      "\n",
      "Epoch 21/100 [0.91s]\n",
      " Train Loss: 0.1846, Acc: 0.9425\n",
      " Val Loss: 0.3223, Acc: 0.8778\n",
      "\n",
      "Epoch 22/100 [0.91s]\n",
      " Train Loss: 0.1826, Acc: 0.9409\n",
      " Val Loss: 0.3225, Acc: 0.8761\n",
      "\n",
      "Epoch 23/100 [0.88s]\n",
      " Train Loss: 0.1815, Acc: 0.9425\n",
      " Val Loss: 0.3240, Acc: 0.8778\n",
      "\n",
      "Epoch 24/100 [0.88s]\n",
      " Train Loss: 0.1794, Acc: 0.9432\n",
      " Val Loss: 0.3241, Acc: 0.8767\n",
      "\n",
      "Epoch 25/100 [0.86s]\n",
      " Train Loss: 0.1781, Acc: 0.9442\n",
      " Val Loss: 0.3233, Acc: 0.8778\n",
      "\n",
      "Epoch 26/100 [0.88s]\n",
      " Train Loss: 0.1762, Acc: 0.9439\n",
      " Val Loss: 0.3229, Acc: 0.8767\n",
      "\n",
      "Epoch 27/100 [0.86s]\n",
      " Train Loss: 0.1750, Acc: 0.9449\n",
      " Val Loss: 0.3229, Acc: 0.8773\n",
      "\n",
      "Epoch 28/100 [0.86s]\n",
      " Train Loss: 0.1751, Acc: 0.9441\n",
      " Val Loss: 0.3234, Acc: 0.8767\n",
      "\n",
      "Epoch 29/100 [0.87s]\n",
      " Train Loss: 0.1737, Acc: 0.9448\n",
      " Val Loss: 0.3242, Acc: 0.8761\n",
      "\n",
      "Epoch 30/100 [0.86s]\n",
      " Train Loss: 0.1724, Acc: 0.9451\n",
      " Val Loss: 0.3235, Acc: 0.8761\n",
      "\n",
      "Epoch 31/100 [0.86s]\n",
      " Train Loss: 0.1719, Acc: 0.9454\n",
      " Val Loss: 0.3234, Acc: 0.8767\n",
      "\n",
      "Early stopping!\n",
      "Final model saved to lstm_model_LSTM_0.8778409090909091.pth\n",
      "\n",
      "Final Model Metrics:\n",
      " Train Loss: 0.1846, Train Acc: 0.9425\n",
      " Val Loss: 0.3223, Val Acc: 0.8778\n",
      "Total training time: 33.40s\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
